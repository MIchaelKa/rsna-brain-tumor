{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elder-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monetary-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-ladder",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "trying-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple3DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "#             nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "            # 13x16x16\n",
    "            \n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "            # 6x8x8\n",
    "            \n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "            # 3x4x4\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(256, 1)\n",
    "        \n",
    "#         self.fc = nn.Linear(256*3*4*4, 1)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "human-career",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, C, D, H, W = 5, 1, 26, 32, 32\n",
    "X = torch.randn(N, C, D, H, W)\n",
    "\n",
    "model = Simple3DNet()\n",
    "output = model(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "atlantic-fifth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2033],\n",
       "        [-0.2078],\n",
       "        [-0.2022],\n",
       "        [-0.2064],\n",
       "        [-0.2061]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-attribute",
   "metadata": {},
   "source": [
    "# Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statutory-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "PATH_TO_DATA = './data/'\n",
    "IMG_PATH_TRAIN = os.path.join(PATH_TO_DATA, 'rsna-brain-tumor-data', 'train')\n",
    "IMG_PATH_TEST = os.path.join(PATH_TO_DATA, 'rsna-brain-tumor-data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decimal-criminal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df = pd.read_csv(PATH_TO_DATA + 'train_labels.csv')\n",
    "train_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "musical-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image3DDataset(Dataset):\n",
    "    def __init__(self, df, path, transform=None):\n",
    "\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = train_labels_df['MGMT_value'][index]\n",
    "        case_id = train_labels_df['BraTS21ID'][index]\n",
    "        case_id = f'{case_id:0>5d}'\n",
    "\n",
    "        # - FLAIR\n",
    "        # - T1w\n",
    "        # - T1wCE\n",
    "        # - T2w\n",
    "        MRI_TYPE = 'FLAIR'\n",
    "        \n",
    "        images_path = os.path.join(IMG_PATH_TRAIN, case_id, MRI_TYPE)\n",
    "\n",
    "        # name = 'Image-100.png'\n",
    "        image_names = sorted(os.listdir(images_path), key=lambda name: int(name[6:][:-4]))\n",
    "        images = []\n",
    "        \n",
    "        for image_name in image_names:\n",
    "            image_path = os.path.join(images_path, image_name)\n",
    "            image = Image.open(image_path)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            # H x W \n",
    "            image = np.array(image).astype(np.float32)\n",
    "            \n",
    "            # image -= 128?\n",
    "            image /= 255\n",
    "            \n",
    "            \n",
    "            # C x H x W\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "                    \n",
    "            images.append(image)\n",
    "            \n",
    "        # C x D x H x W\n",
    "        image_3d = np.stack(images, axis=1)\n",
    "               \n",
    "        return image_3d, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superior-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "def get_train_transform(img_size):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "#         T.ToTensor(),\n",
    "        # T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "danish-password",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Image3DDataset(train_labels_df, IMG_PATH_TRAIN, get_train_transform(IMG_SIZE))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-gather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1, 67, 256, 256), 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_dataset[1]\n",
    "type(image), image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collectible-botswana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65536,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0, 40].reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "finnish-shelter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(image[0, 40].reshape(-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "least-resolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do we need to have equal D for all cases in batch?\n",
    "# looks like yes\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stupid-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "earned-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 143, 256, 256]), torch.Size([1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch, y_batch = next(train_loader_iter)\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "legislative-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9961)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "final-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_batch = y_batch.long()\n",
    "# y_batch = y_batch.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adjusted-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), torch.int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch, y_batch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-niagara",
   "metadata": {},
   "source": [
    "# Single model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "racial-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple3DNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lined-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 5.67 s, total: 34.3 s\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "output = model(x_batch)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unexpected-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "temporal-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.dtype, y_batch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "surprised-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = y_batch.type_as(output)\n",
    "loss = criterion(output.squeeze(1), y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "theoretical-screening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6997, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "continent-winter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0130]], grad_fn=<AddmmBackward>), tensor([1.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "solved-roads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "julian-missile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.argmax(output, 1)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "anonymous-resource",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (output > 0)\n",
    "# y_pred = torch.sigmoid(output)\n",
    "# y_pred = y_pred > 0.5\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "incident-amplifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_samples = torch.sum(y_pred == y_batch)\n",
    "correct_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aware-actress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-movie",
   "metadata": {},
   "source": [
    "# Overfit small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "medical-divide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BraTS21ID</th>\n",
       "      <th>MGMT_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>532</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BraTS21ID  MGMT_value\n",
       "0          9           0\n",
       "1       1009           0\n",
       "2        432           0\n",
       "3        366           1\n",
       "4        731           1\n",
       "5        532           1\n",
       "6        488           1\n",
       "7        309           0\n",
       "8        270           1\n",
       "9        588           0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_number = 10\n",
    "train_df = train_labels_df.sample(frac=1).reset_index(drop=True).head(train_number)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "registered-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Image3DDataset(train_df, IMG_PATH_TRAIN, get_train_transform(IMG_SIZE))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "united-latex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "steady-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_num_iter\n",
    "from utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "specified-saying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "anticipated-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "descending-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    momentum=0.9,\n",
    "    weight_decay=weight_decay,\n",
    "    nesterov=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-brazilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] iter:    0, loss = 0.69925, score = 0.00000, time: 0:00:45\n",
      "[train] iter:    1, loss = 0.69894, score = 0.00000, time: 0:01:32\n",
      "[train] iter:    2, loss = 0.69546, score = 0.33333, time: 0:00:43\n",
      "[train] iter:    3, loss = 0.69609, score = 0.25000, time: 0:00:41\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10\n",
    "print_every = 1\n",
    "\n",
    "train_num_iter(model, device, train_loader, criterion, optimizer, max_iter, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-vermont",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
